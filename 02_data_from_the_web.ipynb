{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNDAMENTALS OF DATA ANALYSIS WITH PYTHON <br><font color=\"crimson\">DAY 2: COLLECTING DATA FROM THE WEB</font>\n",
    "\n",
    "49th [GESIS Spring Seminar: Digital Behavioral Data](https://training.gesis.org/?site=pDetails&pID=0xA33E4024A2554302B3EF4AECFC3484FD)   \n",
    "Cologne, Germany, March 2-6 2010\n",
    "\n",
    "### Course Developers and Instructors \n",
    "\n",
    "* Dr. [John McLevey](www.johnmclevey.com), University of Waterloo (john.mclevey@uwaterloo.ca)     \n",
    "* [Jillian Anderson](https://ca.linkedin.com/in/jillian-anderson-34435714a?challengeId=AQGaFXECVnyVqAAAAW_TLnwJ9VHAlBfinArnfKV6DqlEBpTIolp6O2Bau4MmjzZNgXlHqEIpS5piD4nNjEy0wsqNo-aZGkj57A&submissionId=16582ced-1f90-ec15-cddf-eb876f4fe004), Simon Fraser University (jillianderson8@gmail.com) \n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Overview \n",
    "\n",
    "High-level overview coming soon... \n",
    "\n",
    "### Plan for the Day\n",
    "\n",
    "1. [What you need to know about how the Internet works to collect data from the web](#wyntk)\n",
    "2. [Scraping the Web](#scrape)\n",
    "    * How to scrape text and tables from static websites with BeautifulSoup\n",
    "    * An overview of working with (a) multiple pages and (2) interactive content \n",
    "3. [Collecting data via Application Programming Interfaces](#apis)\n",
    "    * Understanding APIs \n",
    "    * The Guardian API \n",
    "    * The Wikipedia API\n",
    "    * ? The Twitter API ? \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you need to know about how the Internet works to collect data from the web <a id='wyntk'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Web <a id='scrape'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data via Application Programming Interfaces <a id='apis'></a>\n",
    "\n",
    "1. [Understanding APIs](#understanding_apis)\n",
    "3. [The Guardian API](#guardian)   \n",
    "    a. [Overview](#g_overview)      \n",
    "    b. [API Keys](#g_key)      \n",
    "    c. [Making Requests](#g_requests)      \n",
    "    d. [Filtering](#g_filters)    \n",
    "    e. [Extra Information](#g_info)   \n",
    "    f. [Requesting More Results](#g_more)  \n",
    "5. [The Wikipedia API](#wikipedia)   \n",
    "4. The Twitter API\n",
    "5. [Key Points](#key_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='understanding_apis'></a>\n",
    "## Understanding APIs\n",
    "\n",
    "Application Programming Interfaces (APIs) offer an alternative way to access data from online sources. They provide an explicit _interface_ to the data behind the website, defining how you can request data and what format you will receive the data. \n",
    "\n",
    "### Key Components of API Requests & Responses\n",
    "**Endpoints** are the specific web locations where a request for a particular resource can be sent. Usually they have descriptive names like Content, Tweet, User, etc. We communicate with APIs by sending requests to these endpoints, usually in the form of a URL. \n",
    "\n",
    "These URLs usually contain optional **queries**, **parameters**, and **filters** that let us specify exactly what we want the API to return. \n",
    "\n",
    "Once a request has been made to the API it is going to return a **response**. Every response will have a response code, which will indicate whether the request was successful (200) or encountered an error (400, 401, 500, etc.). When you encounter a problem its a good idea to confirm you received a successful response, instead of one of the [many error responses](https://documentation.commvault.com/commvault/v11/article?p=45599.htm). \n",
    "\n",
    "As long as a request was successful, it will return a 200 OK response along with all the requested data. We will delve into what this data looks like below. \n",
    "\n",
    "### APIs vs Web Scraping \n",
    "\n",
    "Benefits: \n",
    "* Structured data (for the most part). \n",
    "* Controlled by an organization or company (Guardian, Twitter, etc) \n",
    "* Documented (usually)\n",
    "* Maintained (usually)\n",
    "* Rules for access are explicitly stated\n",
    "\n",
    "Drawbacks: \n",
    "* Limited to the data made explicitly available\n",
    "* Relies on the organization to make updates\n",
    "* Rate limits & other restrictions apply and are usually based on business reasons rather than technical limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='guardian'></a>\n",
    "## The Guardian API\n",
    "\n",
    "<a id='g_overview'></a>\n",
    "### Overview\n",
    "The Guardian's API allows us to query and download data related to their published articles. \n",
    "\n",
    "The Guardian API has five **endpoints**: \n",
    "* Content (`https://content.guardianapis.com/search`) &mdash; returns content. For dev keys only text. Allows querying and filtering to reduce what is returned.  \n",
    "* Tags &mdash; will return all API tags (> 50, 000). These tags can be used in other quries. \n",
    "* Sections &mdash; logical grouping of content\n",
    "* Editions &mdash; the content for each of the three regional main pages\n",
    "* Single Item &mdash; will return all data related to a specific item (content, tag, or section) in the API. \n",
    "\n",
    "Today, we will focus on the content endpoint. This will allow us to retrieve the body text and metadata for articles published in The Guardian.\n",
    "\n",
    "Often, the easiest way to interface with an API is through a client. In Python, these clients are just packages that provide functions to simplify the process of accessing the API. \n",
    "\n",
    "Alternatively, we can access APIs directly using the [`requests`](https://requests.readthedocs.io/en/master/) library. By accessing the API directly, we maintain freedom in how we use the API, rather than be restricted to a client. This is the option we will choose for interfacing with The Guardian API. \n",
    "\n",
    "<a id='g_key'></a>\n",
    "### API Key\n",
    "Hopefully you were all successful in receiving a Guardian API Key. If not please let one of us know! \n",
    "\n",
    "This API key is what gives you access to the Guardian API. Its kind of like a username and password, all wrapped into one. It is how the API monitors who is accessing their site and makes sure they are abiding by the proper terms of service.\n",
    "\n",
    "We all registered for a developer key. With this key we receive:  \n",
    "* Up to 12 calls per second\n",
    "* Up to 5,000 calls per day\n",
    "* Access to article text (no image, audio, or video)\n",
    "* Access to a subset of Guardian content (1.9 million pieces)\n",
    "\n",
    "If we had registered (and paid) for a commercial key, we would have fewer limitations in what we can access from the API. \n",
    "\n",
    "As I mentioned earlier, you can think of your API token as your username and password for accessing The Guardian API. Like any other credentials, we want to make sure this is kept secure. Most importantly, **never share API tokens in public locations**, including in git repositories or emails. \n",
    "\n",
    "Making an API token public allows others to access the API as if they were you. This puts you at risk if they violate the terms of service you agreed to when you requested an API token. \n",
    "\n",
    "For example, if someone were to get ahold of your API token, they could use it to launch a [denial of service attack](https://en.wikipedia.org/wiki/Denial-of-service_attack) on The Guardian's API. In this case, your token may be revoked and you'd be unable to request a new API token in the future without further violating the terms and services. \n",
    "\n",
    "To mitigate against this problem I would recommend one of two options: \n",
    "* Storing API tokens as environment variables\n",
    "* Creating a `cred.py` to store credentials such as API tokens\n",
    "\n",
    "Personally, I use a `cred.py` containing any of the credentials I need to access APIs, databases, etc. I keep this file stored on my computer in a single location which can be accessed by any Python script on my machine (usually somewhere in `PATH`). This way, the API token is outside of a script I might share and the file is outside of a git repo I might make public one day. \n",
    "\n",
    "If for some reason you need to store the `cred.py` file in the same directory as your Python file and this is within your git repo, make sure to add `cred.py` to the `.gitignore` file.\n",
    "\n",
    "Let's go ahead and create this `cred.py` file now.   \n",
    "\n",
    "Back on the Jupyter Home Page, click on the New button on the upper right side & select the text option (see the screenshot below). \n",
    "\n",
    "<img src=img/new_file.png></img>\n",
    "\n",
    "A new file will open. Rename it cred.py and add the following line, replacing `<YOUR_TOKEN>` with your own API token.  \n",
    "\n",
    "```python3\n",
    "guardian_key = <YOUR_TOKEN>\n",
    "```\n",
    "\n",
    "Save & exit the file.   \n",
    "\n",
    "Run the cell below. If it runs without throwing any errors, the API token has been successfully saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cred\n",
    "\n",
    "api_key = cred.guardian_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='g_requests'></a>\n",
    "### Making API Requests\n",
    "Now that we have our API key stored in a safer location, we can begin making requests to The Guardian API. \n",
    "\n",
    "To start, we will use the `requests` package to make a generic request to the content endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries only needs to be done once\n",
    "import requests\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "\n",
    "MY_PARAMS = {'api-key': api_key}\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "\n",
    "response_dict = response.json()['response']\n",
    "pp.pprint(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a bit of information there...\n",
    "\n",
    "Lets break it down a bit. What are individual fields contained within the response? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these are described in the [content endpoint's documentation](https://open-platform.theguardian.com/documentation/search). We can examine each field individually through indexing our response dictionary. \n",
    "\n",
    "Lets start by seeing what order was used to sort the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['orderBy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, find the total number of items that were returned in this call. Refer to the [documentation](https://open-platform.theguardian.com/documentation/search) if you aren't sure which field you are interested in.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting part of the response is really what is contained within results field. The results will contain the individual items provided by the endpoint. This will be content (mainly news articles) in our case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, examine what is contained within the results field and answer (1) what data structure is being used to store the results (dictionaries, lists, etc.), (2) what data is stored for each result, and (3) how many results were returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='filtering'></a>\n",
    "### Filtering\n",
    "Often we are interested in receiving very specific data from an API, rather than receiving all the data and then sifting through it later on.\n",
    "\n",
    "Luckily, most APIs have built-in ways to make these specifications. In The Guardian's API these are called queries or filters.\n",
    "\n",
    "**Queries** allow you to request content containing free text. This works very similar to a search engine. You can use double quotes to query exact phrase matches and the AND, OR, and NOT operators are supported.   \n",
    "\n",
    "**Filters** allow you to request content based on specific [metadata](https://dataedo.com/kb/data-glossary/what-is-metadata). Once again, you can check the [documentation](https://open-platform.theguardian.com/documentation/search) to see what metadata is available for filtering. \n",
    "\n",
    "We will start off simple. You might have noticed earlier that our response from the API contained the most recent content available. What if we are  actually only interested in retrieving content published prior to Jan 01, 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PARAMS = {'api-key': api_key, \n",
    "             'to-date': '2019-12-31'}\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "\n",
    "response_dict = response.json()['response']\n",
    "pp.pprint(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more parameters to further specify the types of results we want to receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PARAMS = {'api-key': api_key, \n",
    "             'to-date': '2019-12-31', \n",
    "             'from-date': '2015-01-01',\n",
    "             'lang': 'en', \n",
    "             'production-office': 'uk',\n",
    "             'q': '(bees OR bees) AND plants'}\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "\n",
    "response_dict = response.json()['response']\n",
    "pp.pprint(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write an API request to fetch content using a query and at least 2 filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='g_info'></a>\n",
    "### Extra Information\n",
    "You may have noticed in the previous API requests and responses that while we were receiving article URLs, sections, and publication dates, we were missing some pretty important data. Things like headlines, bylines, and body text are not included in the default API response. This additional information is available, but needs to be specified using the `show-fields` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "\n",
    "MY_PARAMS = {'api-key': api_key, \n",
    "             'to-date': '2019-12-31', \n",
    "             'from-date': '2015-01-01',\n",
    "             'lang': 'en', \n",
    "             'production-office': 'uk',\n",
    "             'q': '(bees OR bees) AND plants',\n",
    "             'show-fields': 'wordcount,body,byline'}\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "\n",
    "response_dict = response.json()['response']\n",
    "\n",
    "response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write code to access and print the body text of an article from the `response_dict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='g_more'></a>\n",
    "### Requesting More Results\n",
    "\n",
    "In the API response, there are three fields that relate to the number of results obtained from an API request &mdash; `total`, `pages`, and `pageSize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['pageSize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at them all together, its becomes more clear as to how they relate. \n",
    "\n",
    "* `total` is the number of items available to be returned.  \n",
    "* `pages` is the number of pages available for return, where each page is a small subset of the total number of items.   \n",
    "* `pageSize` is how many items are in the current page being returned.   \n",
    "\n",
    "If its hard to imagine the differences between these values, you can thinking about how Google search results work.   \n",
    "\n",
    "The key point for us to know is that in a basic API request we are likely only receiving a fraction of the total items available for return. If we want to retrieve all the data, we need to look at (1) increasing the page limit and (2) automatically requesting data from the next page. \n",
    "\n",
    "In the cell below, update `MY_PARAMS` to increase the page size from 10 to 50. Use the API [documentation](https://open-platform.theguardian.com/documentation/search) to find the right parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "MY_PARAMS = {'api-key': api_key, \n",
    "             'to-date': '2019-12-31', \n",
    "             'from-date': '2015-01-01',\n",
    "             'lang': 'en', \n",
    "             'production-office': 'uk',\n",
    "             'q': '(bees OR bees) AND plants',\n",
    "             'show-fields': 'wordcount,body,byline'}\n",
    "response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "response_dict = response.json()['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to verify you successfully increased the number of results per page to 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response_dict['pageSize'] < 50:\n",
    "    print('The page size is still less than 50. Try again.')\n",
    "elif response_dict['pageSize'] == 50: \n",
    "    print('The page size is now 50. Good job!')\n",
    "elif response_dict['pageSize'] > 50: \n",
    "    print('The page size is now greater than 50. How did you do that?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each page can display 50 results, nearly 5x fewer pages are needed to contain all of the data we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['pages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still need to find a way to gather data from all the pages, instead of just the first. \n",
    "\n",
    "Luckily, The Guardian API has a built in `page` paramter that allows us to specify which page we want to get results from. We can combine this type of request with a `while` loop to help automate our API requests.   \n",
    "\n",
    "#### Rate Limits\n",
    "Before we look at the code below, we should think about the potential impacts of automating API requests. \n",
    "\n",
    "Remember that with a developer key we are limited to 12 calls per second and 5,000 calls per day. While in this case we will be making very few requests, its important to understand the importance of abiding by these limits. \n",
    "\n",
    "When you sign up for an API token, you typically are required to sign a Terms of Service. These terms are usually (but not always) summarized to make sure the most important information is readily available. This information usually includes: \n",
    "* Rate limits\n",
    "* Disallowed uses\n",
    "* Limitations for sharing data\n",
    "* Intellectual Property considerations\n",
    "\n",
    "While most of these are self-explanatory, its worthwhile taking some time to go over what rate limits are and how they are controlled. \n",
    "\n",
    "Rate limits are the upper bound placed on how many API requests a user can make in a given amount of time. These number differ between websites and even user types. The idea is to limit the rate of requests and ensure the website isn't overrun with traffic. \n",
    "\n",
    "In general, rate limits are controlled in two ways. Some websites will have built-in systems that will detect over-use and throttle or revoke access for a token that is over-requesting. This is the system The Guardian API uses. \n",
    "\n",
    "Other websites rely on the honour system, asking you to abide by your guidelines. In these cases the risk of exceeding limits is higher (since there is no throttling) and if you run a greater risk of being blacklisted if you exceed the API's rate limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Setup\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "MY_PARAMS = {'api-key': api_key, \n",
    "             'to-date': '2019-12-31', \n",
    "             'from-date': '2015-01-01',\n",
    "             'lang': 'en', \n",
    "             'production-office': 'uk',\n",
    "             'q': '(bees OR bees) AND plants',\n",
    "             'show-fields': 'wordcount,body,byline',\n",
    "             'page-size': 50}\n",
    "\n",
    "# Collect All Results\n",
    "all_results = []\n",
    "cur_page = 1\n",
    "total_pages = 1\n",
    "\n",
    "while (cur_page <= total_pages) and (cur_page < 10):  # with a fail safe\n",
    "    # Make a API request\n",
    "    MY_PARAMS['page'] = cur_page\n",
    "    response = requests.get(API_ENDPOINT, params=MY_PARAMS)\n",
    "    response_dict = response.json()['response']\n",
    "\n",
    "    # Update our master results list\n",
    "    all_results += (response_dict['results'])\n",
    "    \n",
    "    # Update our loop variables\n",
    "    total_pages = response_dict['pages']\n",
    "    cur_page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total # of results: {}\".format(len(all_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the results, we can continue to access them and work with them, without having to make more API requests.\n",
    "\n",
    "Whenever possible, **store the results you receive from API requests**. This allows you to access the data without making unneccessary requests to the API. \n",
    "\n",
    "You can store the data in either python variables or in a file. If you are only using the data for a short period of time (e.g. real-time analysis) you can likely get away with using variables within your Python script. \n",
    "\n",
    "However, if you want to access the data after you've finished running your script you should save it to a file. This way the data can be used later in new analyses or to reproduce the work you've already done. \n",
    "\n",
    "Lets store our results in a file, so we can use them later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "FILE_PATH = 'data/guardian_api_results.json'\n",
    "with open(FILE_PATH, 'w') as outfile:\n",
    "    json.dump(all_results, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the results were written in the correct format by reading them back in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## The Wikipedia API\n",
    "The [English Wikipedia API](https://en.wikipedia.org/w/api.php) is one endpoint of the larger [MediaWiki API](https://www.mediawiki.org/wiki/API:Main_page). Other endpoints include the Meta-Wiki, Wikimedia Commons, and German Wikipedia APIs. \n",
    "\n",
    "There is plenty of documentation about how to use these APIs directly, but there is also an easy-to-use Python client we can use. The [`wikipedia`](https://wikipedia.readthedocs.io/en/latest/) Python client developed by Jonathan Goldsmith provides us with functionality for reading and parsing data from Wikipedia. \n",
    "\n",
    "While in the backend `wikipedia` is still using the MediaWiki API, the front-end interface (what we will work with) is much simpler than if we were to use the API directly.\n",
    "\n",
    "### Installing `wikipedia`\n",
    "Likely, up until this point all of the Python packages we've been using have come standard in the Anaconda installation you all have on your machines. \n",
    "\n",
    "However, `wikipedia` is not a default package in either base Python or Anaconda. So, we will need to download it for ourselves. \n",
    "\n",
    "Usually, Python packages can be found on [PyPI](https://pypi.org/), the official repository for Python packages. Any package found on PyPI can be installed using [`pip`](https://pypi.org/project/pip/), Python's package installer. \n",
    "\n",
    "Run the cell below to use `pip`to search PyPI for the `wikipedia` package. \n",
    "\n",
    "> Aside   \n",
    "The `!` at the beginning of the cell tells Jupyter that we want that cell (and that cell only) to be executed on the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 search wikipedia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, the package we are interested in is shown right at the top. We also see that the default version of this package is 1.4.0. To install `wikipedia`, run the cell below. \n",
    "\n",
    "You may notice that some extra packages are being installed, or at least looked for. These packages are _requirements_ of the `wikipedia` packages and need to be installed for `wikipedia` to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wikipedia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you see a message to the effect of `Successfully installed wikipedia-1.4.0` comment out the cell block above to ensure you don't acciudently try to re-install the package. \n",
    "\n",
    "If you get an error message, let one of us know so we can help you debug. \n",
    "\n",
    "Run the cell below to make sure `wikipedia` was installed successfully. If no errors show up, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `wikipedia`\n",
    "Unlike The Guardian or Twitter APIs, Wikipedia's API doesn't require a token. Instead, everything is publically accessible to anyone. \n",
    "\n",
    "\n",
    "We need to be more careful to rate limit. \n",
    "\n",
    "#### Searching\n",
    "Similar to how we search on Wikipedia's website, we can use the API to search for specific content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'spelunking'\n",
    "\n",
    "search_results = wikipedia.search(search_term)\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in a particlar page, we can request it specifically using the `page()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page = wikipedia.page(title=search_results[0])\n",
    "my_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first this result might seem anti-climatic. After all, there really doesn't appear to be any interesting data contained within `my_page`. However, `my_page` actually does contain a lot of information, its just packaged into a `WikipediaPage` object (also known as a class). \n",
    "\n",
    "This object stores data such as the page's summary, links, and categories, all structured neatly within the object. Checkout the [`WikipediaPage` documentation](https://wikipedia.readthedocs.io/en/latest/code.html#wikipedia.WikipediaPage) for a full list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, use a for loop to retrieve and store the summaries for each of the 10 pages in `search_results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jumping Between Pages\n",
    "Links are inherent in Wikipedia. They connect pages to one another and provide a structure for the site. It also means you can almost always get from one page to another through these links. Checkout [Six Degrees of Wikipedia](https://www.sixdegreesofwikipedia.com) if you have any doubts. \n",
    "\n",
    "We can use these links between pages to move page to page, gathering information as we go. The cell below uses the `random` package to select a link at random and display its summary text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function for selecting a random linked page\n",
    "def select_random_link(links):\n",
    "    total_links = len(links)\n",
    "    random_num = random.randrange(0, total_links)\n",
    "    random_page_name = links[random_num]\n",
    "    random_page = wikipedia.page(random_page_name)\n",
    "    return random_page\n",
    "\n",
    "\n",
    "# All links\n",
    "links = my_page.links\n",
    "\n",
    "# Select a random linked page\n",
    "linked_page = select_random_link(links)\n",
    "\n",
    "# Print Results\n",
    "print('There is a link from {} --> {}\\n'.format(my_page.title, \n",
    "                                              linked_page.title))\n",
    "\n",
    "print(\"{}'s summary is\\n {}\".format(linked_page.title,\n",
    "                                    linked_page.summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we took the first step in a [\"random walk\"](https://en.wikipedia.org/wiki/Random_walk) through Wikipedia. In the cell below use the `select_random_link()` function from above and a loop (`while` or `for`) to perform a random walk with 5 steps.\n",
    "\n",
    "Feel free to choose any page as a starting point. Print out the title of each page you visit on the random walk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fairly easy to image how a random walk, left to its own devices, could carry on indefinitely through Wikipedia making API request after API request. If enough people write random walk code, or other code making many requests, its quite possible we could overwhelm the Wikipedia API. \n",
    "\n",
    "When this happens, Wikipedia identifies the IP addresses making the most requests and serves them with an HTTP timeout error. Essentially, Wikipedia punishes the heavy users by returning errors and making them wait until the API is no longer overwhelmed. \n",
    "\n",
    "To help mitigate against this, we can make use of the `set_rate_limiting()` function included in the `wikipedia` Python package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_rate_limiting(rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, any requests we make to the Wikipedia API will be separated by 50 ms (default for the function). If at any point we encounter an HTTP timeout error while using rate limiting, we should adjust the limit using `set_rate_limiting()`'s `min_wait` parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='crimson'> The Twitter API </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='key_points'></a>\n",
    "## Key Points   \n",
    "You should now know: \n",
    "* The differences between working with an API directly and a API client.\n",
    "* The risks associated with sharing API tokens & a method for keeping them out of python scripts. \n",
    "* How to save request results to a file & the importance of doing so. \n",
    "* Why its important to abide by rate limits. \n",
    "* How to install python packages using `pip`.\n",
    "* How to automate API requests using loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
